"use strict";(self.webpackChunkcluster_factory_ce_docs=self.webpackChunkcluster_factory_ce_docs||[]).push([[7182],{720:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>o});var r=s(6672);const l={},t=r.createContext(l);function a(e){const n=r.useContext(t);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:a(e.components),r.createElement(t.Provider,{value:n},e.children)}},889:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>i,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"guides/slurm/deploy-slurm","title":"Deploying SLURM Cluster","description":"image-20220512151655613","source":"@site/docs/guides/60-slurm/01-deploy-slurm.mdx","sourceDirName":"guides/60-slurm","slug":"/guides/slurm/deploy-slurm","permalink":"/docs/guides/slurm/deploy-slurm","draft":false,"unlisted":false,"editUrl":"https://github.com/deepsquare-io/ClusterFactory/tree/main/web/docs/guides/60-slurm/01-deploy-slurm.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{},"sidebar":"docs","previous":{"title":"GitOps with Grendel","permalink":"/docs/guides/provisioning/gitops-with-grendel"},"next":{"title":"Deploy Open OnDemand","permalink":"/docs/guides/slurm/deploy-ondemand"}}');var l=s(3420),t=s(720);s(1538),s(6267);const a={},o="Deploying SLURM Cluster",i={},c=[{value:"Helm and Docker resources",id:"helm-and-docker-resources",level:2},{value:"1. Preparation",id:"1-preparation",level:2},{value:"Namespace and AppProject",id:"namespace-and-appproject",level:3},{value:"LDAP deployment",id:"ldap-deployment",level:3},{value:"SSSD configuration",id:"sssd-configuration",level:3},{value:"MySQL deployment",id:"mysql-deployment",level:3},{value:"JWT Key generation",id:"jwt-key-generation",level:3},{value:"MUNGE Key generation",id:"munge-key-generation",level:3},{value:"2. Begin writing the <code>slurm-cluster-&lt;cluster name&gt;-app.yaml</code>",id:"2-begin-writing-the-slurm-cluster-cluster-name-appyaml",level:2},{value:"2.a. Argo CD Application configuration",id:"2a-argo-cd-application-configuration",level:3},{value:"2.b. Values: Configuring the SLURM cluster",id:"2b-values-configuring-the-slurm-cluster",level:3},{value:"3. Slurm DB Deployment",id:"3-slurm-db-deployment",level:2},{value:"3.a. Secrets",id:"3a-secrets",level:3},{value:"3.b. Values: Enable SLURM DB",id:"3b-values-enable-slurm-db",level:3},{value:"4. Slurm Controller Deployment",id:"4-slurm-controller-deployment",level:2},{value:"4.a. Values: Enable SLURM Controller",id:"4a-values-enable-slurm-controller",level:3},{value:"4.c Testing: <code>sinfo</code> from the controller node",id:"4c-testing-sinfo-from-the-controller-node",level:3},{value:"5. Slurm Compute Bare-Metal Deployment",id:"5-slurm-compute-bare-metal-deployment",level:2},{value:"5.a. Build an OS Image with Slurm",id:"5a-build-an-os-image-with-slurm",level:3},{value:"5.b. Postscripts",id:"5b-postscripts",level:3},{value:"5.c. Reboot the nodes",id:"5c-reboot-the-nodes",level:3},{value:"6. Slurm Login Deployment",id:"6-slurm-login-deployment",level:2},{value:"6.a. Secrets and Volumes",id:"6a-secrets-and-volumes",level:3},{value:"SSH Server configuration",id:"ssh-server-configuration",level:4},{value:"Home directory for the LDAP users",id:"home-directory-for-the-ldap-users",level:4},{value:"6.b. Values: Enable SLURM Login",id:"6b-values-enable-slurm-login",level:3},{value:"6.c Testing: Access to a SLURM Login node",id:"6c-testing-access-to-a-slurm-login-node",level:3}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"deploying-slurm-cluster",children:"Deploying SLURM Cluster"})}),"\n","\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{alt:"image-20220512151655613",src:s(2094).A+"",width:"1065",height:"540"})}),"\n",(0,l.jsx)(n.admonition,{type:"caution",children:(0,l.jsxs)(n.p,{children:["Deploying the SLURM database isn't stable yet. Please feel free to ",(0,l.jsx)(n.a,{href:"https://github.com/deepsquare-io/ClusterFactory/issues/new",children:"create an issue"})," so we can improve its stability."]})}),"\n",(0,l.jsx)(n.h2,{id:"helm-and-docker-resources",children:"Helm and Docker resources"}),"\n",(0,l.jsxs)(n.p,{children:["The Helm resources are stored on ",(0,l.jsx)(n.a,{href:"https://github.com/deepsquare-io/ClusterFactory/tree/main/helm/slurm-cluster",children:"the ClusterFactory Git Repository"}),"."]}),"\n",(0,l.jsxs)(n.p,{children:["The Dockerfile is described in the git repository ",(0,l.jsx)(n.a,{href:"https://github.com/deepsquare-io/slurm-docker",children:"deepsquare-io/slurm-docker"}),"."]}),"\n",(0,l.jsx)(n.p,{children:"The Docker images can be pulled with:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-sh",children:"docker pull ghcr.io/deepsquare-io/slurm:latest-controller\ndocker pull ghcr.io/deepsquare-io/slurm:latest-login\ndocker pull ghcr.io/deepsquare-io/slurm:latest-db\ndocker pull ghcr.io/deepsquare-io/slurm:latest-rest\n"})}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsxs)(n.p,{children:["You should always verify the default Helm ",(0,l.jsx)(n.a,{href:"https://github.com/deepsquare-io/ClusterFactory/blob/main/helm/slurm-cluster/values.yaml",children:"values"})," before editing the ",(0,l.jsx)(n.code,{children:"values"})," field of an Argo CD ",(0,l.jsx)(n.code,{children:"Application"}),"."]})}),"\n",(0,l.jsx)(n.h2,{id:"1-preparation",children:"1. Preparation"}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Compared to the other guides we will start from scratch."})}),"\n",(0,l.jsxs)(n.p,{children:["Delete the ",(0,l.jsx)(n.code,{children:"argo/slurm-cluster"})," directory (or rename it)."]}),"\n",(0,l.jsx)(n.p,{children:"Deploying a SLURM cluster isn't easy and you MUST have these components ready:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"A LDAP server and a SSSD configuration, to synchronize the user ID across the cluster"}),"\n",(0,l.jsx)(n.li,{children:"A MySQL server for the SLURM DB"}),"\n",(0,l.jsx)(n.li,{children:"A JWT private key, for the authentication via REST API"}),"\n",(0,l.jsx)(n.li,{children:"A MUNGE key, for the authentication of SLURM daemons"}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"namespace-and-appproject",children:"Namespace and AppProject"}),"\n",(0,l.jsxs)(n.p,{children:["Create and apply the ",(0,l.jsx)(n.code,{children:"Namespace"})," and ",(0,l.jsx)(n.code,{children:"AppProject"}),":"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-yaml",metastring:'title="argo/slurm-cluster/namespace.yaml"',children:"apiVersion: v1\nkind: Namespace\nmetadata:\n  name: slurm-cluster\n  labels:\n    app.kubernetes.io/name: slurm-cluster\n"})}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-yaml",metastring:'title="argo/slurm-cluster/app-project.yaml"',children:"apiVersion: argoproj.io/v1alpha1\nkind: AppProject\nmetadata:\n  name: slurm-cluster\n  namespace: argocd\n  # Finalizer that ensures that project is not deleted until it is not referenced by any application\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io\nspec:\n  description: Slurm cluster\n  # Allow manifests to deploy from any Git repos\n  sourceRepos:\n    - '*'\n  # Only permit applications to deploy to the namespace in the same cluster\n  destinations:\n    - namespace: slurm-cluster\n      server: https://kubernetes.default.svc\n\n  namespaceResourceWhitelist:\n    - kind: '*'\n      group: '*'\n\n  clusterResourceWhitelist:\n    - kind: '*'\n      group: '*'\n"})}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",metastring:'title="user@local:/ClusterFactory"',children:"kubectl apply -f argo/slurm-cluster/\n"})}),"\n",(0,l.jsx)(n.h3,{id:"ldap-deployment",children:"LDAP deployment"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["Follow ",(0,l.jsx)(n.a,{href:"/docs/guides/deploy-ldap",children:"the guide"}),"."]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["Open a shell on the LDAP server container and create a user and a group ",(0,l.jsx)(n.code,{children:"slurm"}),":"]}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",metastring:'title="Pod: dirsrv-389ds-0 (namespace: ldap)"',children:'# Create user\ndsidm -b "dc=example,dc=com" localhost user create \\\n  --uid slurm \\\n  --cn slurm \\\n  --displayName slurm \\\n  --homeDirectory "/dev/shm" \\\n  --uidNumber 1501 \\\n  --gidNumber 1501\n\n# Create group\ndsidm -b "dc=example,dc=com" localhost group create \\\n  --cn slurm\n\n# Add posixGroup property and gidNumber\ndsidm -b "dc=example,dc=com" localhost group modify slurm \\\n  "add:objectClass:posixGroup" \\\n  "add:gidNumber:1501"\n'})}),"\n",(0,l.jsx)(n.h3,{id:"sssd-configuration",children:"SSSD configuration"}),"\n",(0,l.jsxs)(n.p,{children:["Let's store it in a ",(0,l.jsx)(n.code,{children:"Secret"}),":"]}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["Create the ",(0,l.jsx)(n.code,{children:"argo/slurm-cluster/secrets/"})," directory and create a ",(0,l.jsx)(n.code,{children:"-secret.yaml.local"})," file:"]}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-yaml",metastring:'title="argo/slurm-cluster/secrets/sssd-secret.yaml.local"',children:'apiVersion: v1\nkind: Secret\nmetadata:\n  name: sssd-secret\n  namespace: slurm-cluster\ntype: Opaque\nstringData:\n  sssd.conf: |\n    # https://sssd.io/docs/users/troubleshooting/how-to-troubleshoot-backend.html\n    [sssd]\n    services = nss,pam,sudo,ssh\n    config_file_version = 2\n    domains = example-ldap\n\n    [sudo]\n\n    [nss]\n\n    [pam]\n    offline_credentials_expiration = 60\n\n    [domain/example-ldap]\n    debug_level=3\n    cache_credentials = True\n    dns_resolver_timeout = 15\n\n    override_homedir = /home/ldap-users/%u\n\n    id_provider = ldap\n    auth_provider = ldap\n    access_provider = ldap\n    chpass_provider = ldap\n\n    ldap_schema = rfc2307bis\n\n    ldap_uri = ldaps://dirsrv-389ds.ldap.svc.cluster.local:3636\n    ldap_default_bind_dn = cn=Directory Manager\n    ldap_default_authtok = <password>\n    ldap_search_timeout = 50\n    ldap_network_timeout = 60\n    ldap_user_member_of = memberof\n    ldap_user_gecos = cn\n    ldap_user_uuid = nsUniqueId\n    ldap_group_uuid = nsUniqueId\n\n    ldap_search_base = ou=people,dc=example,dc=com\n    ldap_group_search_base = ou=groups,dc=example,dc=com\n    ldap_sudo_search_base = ou=sudoers,dc=example,dc=com\n    ldap_user_ssh_public_key = nsSshPublicKey\n\n    ldap_account_expire_policy = rhds\n    ldap_access_order = filter, expire\n    ldap_access_filter = (objectClass=posixAccount)\n\n    ldap_tls_cipher_suite = HIGH\n    # On Ubuntu, the LDAP client is linked to GnuTLS instead of OpenSSL => cipher suite names are different\n    # In fact, it\'s not even a cipher suite name that goes here, but a so called "priority list" => see $> gnutls-cli --priority-list\n    # See https://backreference.org/2009/11/18/openssl-vs-gnutls-cipher-names/ , gnutls-cli is part of package gnutls-bin\n'})}),"\n",(0,l.jsx)(n.p,{children:"Adapt this secret based on your LDAP configuration."}),"\n",(0,l.jsxs)(n.ol,{start:"2",children:["\n",(0,l.jsx)(n.li,{children:"Seal the secret:"}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",metastring:'title="user@local:/ClusterFactory"',children:"cfctl kubeseal\n"})}),"\n",(0,l.jsxs)(n.ol,{start:"3",children:["\n",(0,l.jsx)(n.li,{children:"Apply the SealedSecret:"}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",metastring:'title="user@local:/ClusterFactory"',children:"kubectl apply -f argo/slurm-cluster/secrets/sssd-sealed-secret.yaml\n"})}),"\n",(0,l.jsx)(n.h3,{id:"mysql-deployment",children:"MySQL deployment"}),"\n",(0,l.jsxs)(n.p,{children:["You can deploy MySQL using the ",(0,l.jsx)(n.a,{href:"https://bitnami.com/stack/mysql/helm",children:"Helm Chart of Bitnami"})," and deploy it with ",(0,l.jsx)(n.a,{href:"https://argo-cd.readthedocs.io/en/stable/operator-manual/declarative-setup/",children:"ArgoCD"}),"."]}),"\n",(0,l.jsx)(n.p,{children:"After deploying the MySQL/MariaDB server, you must create a slurm database. Open a shell on the MySQL container and run:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",metastring:'title="Pod: mariadb-0"',children:"mysql -u root -p -h localhost\n# Enter your root password\n"})}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-sql",metastring:'title="Pod: mariadb-0 (sql)"',children:"create user 'slurm'@'%' identified by '<your password>';\ngrant all on slurm_acct_db.* TO 'slurm'@'%';\ncreate database slurm_acct_db;\n"})}),"\n",(0,l.jsx)(n.h3,{id:"jwt-key-generation",children:"JWT Key generation"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",metastring:'title="user@local"',children:"ssh-keygen -t rsa -b 4096 -m PEM -f jwtRS256.key\n"})}),"\n",(0,l.jsxs)(n.p,{children:["Let's store it in a ",(0,l.jsx)(n.code,{children:"Secret"}),":"]}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["Create a ",(0,l.jsx)(n.code,{children:"-secret.yaml.local"})," file:"]}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-yaml",metastring:'title="argo/slurm-cluster/secrets/slurm-secret.yaml.local"',children:"apiVersion: v1\nkind: Secret\nmetadata:\n  name: slurm-secret\n  namespace: slurm-cluster\ntype: Opaque\nstringData:\n  jwt_hs256.key: |\n    -----BEGIN RSA PRIVATE KEY-----\n    ...\n    -----END RSA PRIVATE KEY-----\n"})}),"\n",(0,l.jsxs)(n.ol,{start:"2",children:["\n",(0,l.jsx)(n.li,{children:"Seal the secret:"}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",metastring:'title="user@local:/ClusterFactory"',children:"cfctl kubeseal\n"})}),"\n",(0,l.jsxs)(n.ol,{start:"3",children:["\n",(0,l.jsx)(n.li,{children:"Apply the SealedSecret:"}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",metastring:'title="user@local:/ClusterFactory"',children:"kubectl apply -f argo/slurm-cluster/secrets/slurm-sealed-secret.yaml\n"})}),"\n",(0,l.jsx)(n.h3,{id:"munge-key-generation",children:"MUNGE Key generation"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",metastring:'title="root@local"',children:"# As root\ndnf install -y munge\n/usr/sbin/create-munge-key\ncat /etc/munge/munge.key | base64\n"})}),"\n",(0,l.jsxs)(n.p,{children:["Let's store it in a ",(0,l.jsx)(n.code,{children:"Secret"}),":"]}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["Create a ",(0,l.jsx)(n.code,{children:"-secret.yaml.local"})," file:"]}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-yaml",metastring:'title="argo/slurm-cluster/secrets/munge-secret.yaml.local"',children:"apiVersion: v1\nkind: Secret\nmetadata:\n  name: munge-secret\n  namespace: slurm-cluster\ntype: Opaque\ndata:\n  munge.key: |\n    <base 64 encoded key>\n"})}),"\n",(0,l.jsxs)(n.ol,{start:"2",children:["\n",(0,l.jsx)(n.li,{children:"Seal the secret:"}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",metastring:'title="user@local:/ClusterFactory"',children:"cfctl kubeseal\n"})}),"\n",(0,l.jsxs)(n.ol,{start:"3",children:["\n",(0,l.jsx)(n.li,{children:"Apply the SealedSecret:"}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",metastring:'title="user@local:/ClusterFactory"',children:"kubectl apply -f argo/cvmfs/secrets/munge-sealed-secret.yaml\n"})}),"\n",(0,l.jsxs)(n.h2,{id:"2-begin-writing-the-slurm-cluster-cluster-name-appyaml",children:["2. Begin writing the ",(0,l.jsx)(n.code,{children:"slurm-cluster-<cluster name>-app.yaml"})]}),"\n",(0,l.jsx)(n.h3,{id:"2a-argo-cd-application-configuration",children:"2.a. Argo CD Application configuration"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-yaml",metastring:'title="argo/slurm-cluster/apps/slurm-cluster-<cluster name>-app.yaml"',children:'apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: slurm-cluster-<FILL ME: cluster name>-app\n  namespace: argocd\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io\nspec:\n  project: slurm-cluster\n  source:\n   # You should have forked this repo. Change the URL to your fork.\n    repoURL: git@github.com:<FILL ME: your account>/ClusterFactory.git\n    # You should use your branch too.\n    targetRevision: HEAD\n    path: helm/slurm-cluster\n    helm:\n      releaseName: slurm-cluster-<FILL ME: cluster name>\n\n      valueFiles:\n        - values-<FILL ME: cluster name>.yaml\n\n  destination:\n    server: \'https://kubernetes.default.svc\'\n    namespace: slurm-cluster\n\n  syncPolicy:\n    automated:\n      prune: true # Specifies if resources should be pruned during auto-syncing ( false by default ).\n      selfHeal: true # Specifies if partial app sync should be executed when resources are changed only in target Kubernetes cluster and no git change detected ( false by default ).\n      allowEmpty: false # Allows deleting all application resources during automatic syncing ( false by default ).\n    syncOptions: []\n    retry:\n      limit: 5 # number of failed sync attempt retries; unlimited number of attempts if less than 0\n      backoff:\n        duration: 5s # the amount to back off. Default unit is seconds, but could also be a duration (e.g. "2m", "1h")\n        factor: 2 # a factor to multiply the base duration after each failed retry\n        maxDuration: 3m # the maximum amount of time allowed for the backoff strategy\n'})}),"\n",(0,l.jsx)(n.h3,{id:"2b-values-configuring-the-slurm-cluster",children:"2.b. Values: Configuring the SLURM cluster"}),"\n",(0,l.jsx)(n.p,{children:"Add:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-yaml",metastring:'title="helm/slurm-cluster/values-<cluster name>.yaml"',children:'sssd:\n  secretName: sssd-secret\n\nmunge:\n  secretName: munge-secret\n\njwt:\n  secretName: slurm-secret\n\nslurmConfig:\n  clusterName: <FILL ME: cluster-name>\n\n  compute:\n    srunPortRangeStart: 60001\n    srunPortRangeEnd: 63000\n    debug: debug5\n\n  accounting: |\n    AccountingStorageType=accounting_storage/slurmdbd\n    AccountingStorageHost=slurm-cluster-<FILL ME: cluster name>.slurm-cluster.svc.cluster.local\n    AccountingStoragePort=6819\n    AccountingStorageTRES=gres/gpu\n\n  controller:\n    parameters: enable_configless\n    debug: debug5\n\n  defaultResourcesAllocation: |\n    # Change accordingly\n    DefCpuPerGPU=4\n    DefMemPerCpu=7000\n\n  nodes: |\n    # Change accordingly\n    NodeName=cn[1-12]  CPUs=32 Boards=1 SocketsPerBoard=1 CoresPerSocket=16 ThreadsPerCore=2 RealMemory=128473 Gres=gpu:4\n\n  partitions: |\n    # Change accordingly\n    PartitionName=main Nodes=cn[1-12] Default=YES MaxTime=INFINITE State=UP OverSubscribe=NO TRESBillingWeights="CPU=2.6,Mem=0.25G,GRES/gpu=24.0"\n\n  gres: |\n    # Change accordingly\n    NodeName=cn[1-12] File=/dev/nvidia[0-3] AutoDetect=nvml\n\n  # Extra slurm.conf configuration\n  extra: |\n    LaunchParameters=enable_nss_slurm\n    DebugFlags=Script,Gang,SelectType\n    TCPTimeout=5\n\n    # MPI stacks running over Infiniband or OmniPath require the ability to allocate more\n    # locked memory than the default limit. Unfortunately, user processes on login nodes\n    # may have a small memory limit (check it by ulimit -a) which by default are propagated\n    # into Slurm jobs and hence cause fabric errors for MPI.\n    PropagateResourceLimitsExcept=MEMLOCK\n\n    ProctrackType=proctrack/cgroup\n    TaskPlugin=task/cgroup\n    SwitchType=switch/none\n    MpiDefault=pmix_v2\n    ReturnToService=2\n    GresTypes=gpu\n    PreemptType=preempt/qos\n    PreemptMode=REQUEUE\n    PreemptExemptTime=-1\n    Prolog=/etc/slurm/prolog.d/*\n    Epilog=/etc/slurm/epilog.d/*\n\n    # Federation\n    FederationParameters=fed_display\n'})}),"\n",(0,l.jsx)(n.h2,{id:"3-slurm-db-deployment",children:"3. Slurm DB Deployment"}),"\n",(0,l.jsx)(n.h3,{id:"3a-secrets",children:"3.a. Secrets"}),"\n",(0,l.jsxs)(n.p,{children:["Assuming you have deployed LDAP and MySQL, we will store the ",(0,l.jsx)(n.code,{children:"slurmdbd.conf"})," inside a secret:"]}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["Create a ",(0,l.jsx)(n.code,{children:"-secret.yaml.local"})," file:"]}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-yaml",metastring:'title="argo/slurm-cluster/secrets/slurmdbd-conf-secret.yaml.local"',children:"apiVersion: v1\nkind: Secret\nmetadata:\n  name: slurmdbd-conf-secret\n  namespace: slurm-cluster\ntype: Opaque\nstringData:\n  slurmdbd.conf: |\n    # See https://slurm.schedmd.com/slurmdbd.conf.html\n    ### Main\n    DbdHost=slurm-cluster-<FILL ME: cluster name>-db-0\n    SlurmUser=slurm\n\n    ### Logging\n    DebugLevel=debug5\t# optional, defaults to 'info'. Possible values: fatal, error, info, verbose, debug, debug[2-5]\n    LogFile=/var/log/slurm/slurmdbd.log\n    PidFile=/var/run/slurmdbd.pid\n    LogTimeFormat=thread_id\n\n    AuthAltTypes=auth/jwt\n    AuthAltParameters=jwt_key=/var/spool/slurm/jwt_hs256.key\n\n    ### Database server configuration\n    StorageType=accounting_storage/mysql\n    StorageHost=<FILL ME>\n    StorageUser=<FILL ME>\n    StoragePass=<FILL ME>\n"})}),"\n",(0,l.jsxs)(n.p,{children:["Replace the ",(0,l.jsx)(n.code,{children:"<FILL ME>"})," according to your existing configuration."]}),"\n",(0,l.jsxs)(n.ol,{start:"2",children:["\n",(0,l.jsx)(n.li,{children:"Seal the secret:"}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",metastring:'title="user@local:/ClusterFactory"',children:"cfctl kubeseal\n"})}),"\n",(0,l.jsxs)(n.ol,{start:"3",children:["\n",(0,l.jsx)(n.li,{children:"Apply the SealedSecret:"}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",metastring:'title="user@local:/ClusterFactory"',children:"kubectl apply -f argo/slurm-cluster/secrets/slurmdbd-conf-sealed-secret.yaml\n"})}),"\n",(0,l.jsx)(n.h3,{id:"3b-values-enable-slurm-db",children:"3.b. Values: Enable SLURM DB"}),"\n",(0,l.jsxs)(n.p,{children:["Edit the ",(0,l.jsx)(n.code,{children:"elm/slurm-cluster/values-<cluster name>.yaml"})," values"]}),"\n",(0,l.jsx)(n.p,{children:"Let's add the values to deploy a SLURM DB."}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-yaml",metastring:'title="helm/slurm-cluster/values-<cluster name>.yaml"',children:"db:\n  enabled: true\n\n  config:\n    secretName: slurmdbd-conf-secret\n"})}),"\n",(0,l.jsx)(n.p,{children:"If you are using LDAPS and the CA is private:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-yaml",metastring:'title="helm/slurm-cluster/values-<cluster name>.yaml"',children:"db:\n  enabled: true\n\n  config:\n    secretName: slurmdbd-conf-secret\n\n  command: ['sh', '-c', 'update-ca-trust && /init']\n\n  volumeMounts:\n    - name: ca-cert\n      mountPath: /etc/pki/ca-trust/source/anchors/example.com.ca.pem\n      subPath: example.com.ca.pem\n\n  volumes:\n    - name: ca-cert\n      secret:\n        secretName: local-ca-secret\n"})}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.code,{children:"local-ca-secret"})," is a Secret containing ",(0,l.jsx)(n.code,{children:"example.com.ca.pem"}),"."]}),"\n",(0,l.jsx)(n.p,{children:"You can already deploy it:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",children:'git add .\ngit commit -m "Added SLURM DB values"\ngit push\n'})}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",metastring:'title="user@local:/ClusterFactory"',children:"# This is optional if the application is already deployed.\nkubectl apply -f argo/slurm-cluster/apps/slurm-cluster-<cluster name>-app.yaml\n"})}),"\n",(0,l.jsxs)(n.p,{children:["The service should be accessible at the address ",(0,l.jsx)(n.code,{children:"slurm-cluster-<cluster name>-db-0.slurm-cluster.svc.cluster.local"}),". Use that URL in the slurm config."]}),"\n",(0,l.jsx)(n.h2,{id:"4-slurm-controller-deployment",children:"4. Slurm Controller Deployment"}),"\n",(0,l.jsx)(n.h3,{id:"4a-values-enable-slurm-controller",children:"4.a. Values: Enable SLURM Controller"}),"\n",(0,l.jsx)(n.p,{children:"Let's add the values to deploy a SLURM Controller."}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-yaml",metastring:'title="helm/slurm-cluster/values-<cluster name>.yaml"',children:"controller:\n  enabled: true\n\n  persistence:\n    storageClassName: 'dynamic-nfs'\n    accessModes: ['ReadWriteOnce']\n    size: 10Gi\n\n  nodeSelector:\n    topology.kubernetes.io/region: <FILL ME> # <country code>-<city>\n    topology.kubernetes.io/zone: <FILL ME> # <country code>-<city>-<index>\n\n  resources:\n    requests:\n      cpu: '250m'\n      memory: '1Gi'\n    limits:\n      cpu:\n      memory: '1Gi'\n"})}),"\n",(0,l.jsxs)(n.p,{children:["Notice that ",(0,l.jsx)(n.code,{children:"kubernetes.io/hostname"})," is used, this is because the slurm controller will be using the host network and we don't want to make the slurm controller move around."]}),"\n",(0,l.jsx)(n.p,{children:"We might develop a HA setup in the future version of ClusterFactory."}),"\n",(0,l.jsx)(n.p,{children:"If you are using LDAPS and the CA is private, append these values:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-yaml",metastring:'title="helm/slurm-cluster/values-<cluster name>.yaml"',children:"controller:\n  # ...\n  command: ['sh', '-c', 'update-ca-trust && /init']\n\n  volumeMounts:\n    - name: ca-cert\n      mountPath: /etc/pki/ca-trust/source/anchors/example.com.ca.pem\n      subPath: example.com.ca.pem\n\n  volumes:\n    - name: ca-cert\n      secret:\n        secretName: local-ca-secret\n"})}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.code,{children:"local-ca-secret"})," is a Secret containing ",(0,l.jsx)(n.code,{children:"example.com.ca.pem"}),"."]}),"\n",(0,l.jsx)(n.p,{children:"You can already deploy it:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",children:'git add .\ngit commit -m "Added SLURM Controller values"\ngit push\n'})}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",metastring:'title="user@local:/ClusterFactory"',children:"# This is optional if the application is already deployed.\nkubectl apply -f argo/slurm-cluster/apps/slurm-cluster-<cluster name>-app.yaml\n"})}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsxs)(n.p,{children:["The SLURM controller is in host mode using ",(0,l.jsx)(n.code,{children:"hostPort"})," so it can communicate with the bare-metal hosts. There\nis also a SLURM controller ",(0,l.jsx)(n.code,{children:"Service"})," running for the internal communication with Slurm DB and Slurm Login."]})}),"\n",(0,l.jsxs)(n.h3,{id:"4c-testing-sinfo-from-the-controller-node",children:["4.c Testing: ",(0,l.jsx)(n.code,{children:"sinfo"})," from the controller node"]}),"\n",(0,l.jsxs)(n.p,{children:["You should be able to run a ",(0,l.jsx)(n.code,{children:"kubectl exec"})," session on the controller node and execute ",(0,l.jsx)(n.code,{children:"sinfo"}),":"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",metastring:'title="user@local"',children:"[user@local /]> kubectl exec -it -n slurm-cluster slurm-cluster-<cluster-name>-controller-0 -c slurm-cluster-<cluster-name>-controller -- bash\n\n[root@slurm-cluster-reindeer-controller-0 /]> sinfo\nPARTITION   AVAIL  TIMELIMIT  NODES  STATE NODELIST\nmain*          up   infinite     12  down* cn[1-12]\n"})}),"\n",(0,l.jsx)(n.h2,{id:"5-slurm-compute-bare-metal-deployment",children:"5. Slurm Compute Bare-Metal Deployment"}),"\n",(0,l.jsx)(n.h3,{id:"5a-build-an-os-image-with-slurm",children:"5.a. Build an OS Image with Slurm"}),"\n",(0,l.jsxs)(n.p,{children:["We have enabled ",(0,l.jsx)(n.code,{children:"config-less"})," in the ",(0,l.jsx)(n.code,{children:"slurm.conf"}),"."]}),"\n",(0,l.jsx)(n.p,{children:"We need to build an OS Image with Slurm Daemon installed."}),"\n",(0,l.jsx)(n.p,{children:"Install these packages:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"pmix4\nslurm\nslurm-contribs\nslurm-libpmi\nslurm-pam_slurm\nslurm-slurmd\n\nlibnvidia-container1\nlibnvidia-container-tools\nenroot-hardened\nenroot-hardened+caps\nnvslurm-plugin-pyxis\n"})}),"\n",(0,l.jsxs)(n.p,{children:["from the ",(0,l.jsx)(n.a,{href:"https://yum.deepsquare.run/yum.repo",children:"DeepSquare YUM repository"}),"."]}),"\n",(0,l.jsx)(n.h3,{id:"5b-postscripts",children:"5.b. Postscripts"}),"\n",(0,l.jsx)(n.p,{children:"Next, you have to configure a service by using a postscript."}),"\n",(0,l.jsx)(n.p,{children:"The service:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-properties",metastring:'title="/etc/systemd/system/slurmd.service"',children:"[Unit]\nDescription=Slurm node daemon\nAfter=network.target munge.service\n\n[Service]\nType=forking\nExecStartPre=/usr/bin/id slurm\nRestart=always\nRestartSec=3\nExecStart=/usr/sbin/slurmd -d /usr/sbin/slurmstepd --conf-server slurm-cluster-<cluster name>-controller-0.example.com\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/var/run/slurmd.pid\nKillMode=process\nLimitNOFILE=51200\nLimitMEMLOCK=infinity\nLimitSTACK=infinity\n\n[Install]\nWantedBy=multi-user.target\n"})}),"\n",(0,l.jsxs)(n.admonition,{type:"warning",children:[(0,l.jsxs)(n.p,{children:["Add ",(0,l.jsx)(n.code,{children:"slurm-cluster-<cluster name>-controller-0.example.com"})," to the CoreDNS configuration."]}),(0,l.jsx)(n.p,{children:"If you are using a hostPort, put the IP of the Kubernetes host hosting the pod.\nIf you are using a LoadBalancer, put the IP you've given to the LoadBalancer.\nIf you are using IPVLAN, put the IP you've given to the IPVLAN."})]}),"\n",(0,l.jsx)(n.p,{children:"A simple postbootscript:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",metastring:'title="sample-configure-slurm.sh',children:"#!/bin/sh -ex\n# Copy the CA certificate created for the private-cluster-issuer\ncp ./certs/my-ca.prem /etc/pki/ca-trust/source/anchors/my-ca.pem\n\nmkdir -p /var/log/slurm/\n\ncat <<\\END | base64 -d >/etc/munge/munge.key\n...\nEND\n\nchmod 600 /etc/munge/munge.key\n\ncat <<\\END >/etc/sssd/sssd.conf\n...\nEND\n\nchmod 600 /etc/sssd/sssd.conf\n\n#-- Add enroot extra hooks for PMIx and PyTorch multi-node support\ncp /usr/share/enroot/hooks.d/50-slurm-pmi.sh /usr/share/enroot/hooks.d/50-slurm-pytorch.sh /etc/enroot/hooks.d\n\n# Enable Pyxis (container jobs)\ncat <<\\END >/etc/slurm/plugstack.conf.d/\noptional /usr/lib64/slurm/spank_pyxis.so runtime_path=/run/pyxis container_scope=job\nEND\n\ncat <<\\END >/etc/systemd/system/slurmd.service\n[Unit]\nDescription=Slurm node daemon\nAfter=network.target munge.service remote-fs.target\nWants=network-online.target\n\n[Service]\nType=simple\nRestart=always\nRestartSec=3\nExecStart=/usr/sbin/slurmd -d /usr/sbin/slurmstepd --conf-server <node host IP>\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/var/run/slurmd.pid\nKillMode=process\nLimitNOFILE=131072\nLimitMEMLOCK=infinity\nLimitSTACK=infinity\nDelegate=yes\n\nStandardOutput=null\nStandardError=null\n\n[Install]\nWantedBy=multi-user.target\nEND\n\n#-- Wait for LDAP\nupdate-ca-trust\nsystemctl restart sssd\nwhile ! id slurm\ndo\n  sleep 1\ndone\n\nsystemctl daemon-reload\nsystemctl restart munge\nsystemctl enable slurmd\nsystemctl start --no-block slurmd\n"})}),"\n",(0,l.jsx)(n.p,{children:"After setup SLURM, you should also:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Mount the home directory of the LDAP users (probably like ",(0,l.jsx)(n.code,{children:"/home/ldap-users"}),")"]}),"\n",(0,l.jsx)(n.li,{children:"Use the postscript to configure SSSD"}),"\n",(0,l.jsxs)(n.li,{children:["Use the postscript to import the ",(0,l.jsx)(n.code,{children:"munge.key"})]}),"\n"]}),"\n",(0,l.jsxs)(n.admonition,{title:"Troubleshooting",type:"warning",children:[(0,l.jsx)(n.p,{children:"In the order:"}),(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["Check your journal (",(0,l.jsx)(n.code,{children:"journalctl"}),") and check the logs."]}),"\n",(0,l.jsxs)(n.li,{children:["Stuck in a ",(0,l.jsx)(n.code,{children:"id slurm"})," loop ?","\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Check the SSSD configuration"}),"\n",(0,l.jsx)(n.li,{children:"Check the TLS certificate"}),"\n",(0,l.jsxs)(n.li,{children:["Check if Traefik and LDAP ports (",(0,l.jsx)(n.code,{children:"nc -vz <LB IP> <LDAP PORT>"}),")"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["SSSD is working (",(0,l.jsx)(n.code,{children:"id slurm"})," shows 1501 as UID), but ",(0,l.jsx)(n.code,{children:"sinfo"})," is crashing","\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Check the health of your SLURM controller pod"}),"\n",(0,l.jsxs)(n.li,{children:["Check if the ports of the SLURM controller (",(0,l.jsx)(n.code,{children:"nc -vz <SLURM CTL IP> 6817"}),")"]}),"\n",(0,l.jsxs)(n.li,{children:["Check if the domain name of the SLURM controller can be received (",(0,l.jsx)(n.code,{children:"dig @<DNS SERVER IP> slurm-cluster-<cluster name>-controller-0.example.com"}),")"]}),"\n",(0,l.jsxs)(n.li,{children:["Check the DNS client configuration (",(0,l.jsx)(n.code,{children:"/etc/resolv.conf"}),")"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["Slurm is crashing but not ",(0,l.jsx)(n.code,{children:"sinfo"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Check ",(0,l.jsx)(n.code,{children:"/var/log/slurm/slurm.log"})]}),"\n"]}),"\n"]}),"\n"]})]}),"\n",(0,l.jsx)(n.h3,{id:"5c-reboot-the-nodes",children:"5.c. Reboot the nodes"}),"\n",(0,l.jsxs)(n.p,{children:["If the controller is running, the nodes should automatically receive the ",(0,l.jsx)(n.code,{children:"slurm.conf"})," inside ",(0,l.jsx)(n.code,{children:"/run/slurm/conf"}),"."]}),"\n",(0,l.jsx)(n.h2,{id:"6-slurm-login-deployment",children:"6. Slurm Login Deployment"}),"\n",(0,l.jsx)(n.h3,{id:"6a-secrets-and-volumes",children:"6.a. Secrets and Volumes"}),"\n",(0,l.jsx)(n.h4,{id:"ssh-server-configuration",children:"SSH Server configuration"}),"\n",(0,l.jsxs)(n.p,{children:["The login nodes can be exposed to the external network using Multus CNI and the IPVLAN plugin. This is to expose the ",(0,l.jsx)(n.code,{children:"srunPortRange"})," and the SSH port."]}),"\n",(0,l.jsxs)(n.p,{children:["If you don't plan to use ",(0,l.jsx)(n.code,{children:"srun"})," and prefer ",(0,l.jsx)(n.code,{children:"sbatch"}),", we recommend to use a simple Kubernetes Service to expose the login nodes."]}),"\n",(0,l.jsx)(n.p,{children:"Thanks to SSSD, the users can log in to the nodes using SSH using the passwords stored in LDAP."}),"\n",(0,l.jsx)(n.p,{children:"We have to generate the SSH host keys:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",metastring:'title="user@local"',children:"yes 'y' | ssh-keygen -N '' -f ./ssh_host_rsa_key -t rsa -C login-node\nyes 'y' | ssh-keygen -N '' -f ./ssh_host_ecdsa_key -t ecdsa -C login-node\nyes 'y' | ssh-keygen -N '' -f ./ssh_host_ed25519_key -t ed25519 -C login-node\n"})}),"\n",(0,l.jsxs)(n.p,{children:["6 files will be generated. We will also add our ",(0,l.jsx)(n.code,{children:"sshd_config"}),"."]}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["Create a ",(0,l.jsx)(n.code,{children:"-secret.yaml.local"})," file:"]}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-yaml",metastring:'title="argo/slurm-cluster/secrets/login-sshd-secret.yaml.local"',children:"apiVersion: v1\nkind: Secret\nmetadata:\n  name: login-sshd-secret\n  namespace: slurm-cluster\ntype: Opaque\nstringData:\n  ssh_host_ecdsa_key: |\n    -----BEGIN OPENSSH PRIVATE KEY-----\n    <FILL ME>\n    -----END OPENSSH PRIVATE KEY-----\n  ssh_host_ecdsa_key.pub: |\n    ecdsa-sha2-nistp256 <FILL ME>\n  ssh_host_ed25519_key: |\n    -----BEGIN OPENSSH PRIVATE KEY-----\n    <FILL ME>\n    -----END OPENSSH PRIVATE KEY-----\n  ssh_host_ed25519_key.pub: |\n    ssh-ed25519 <FILL ME>\n  ssh_host_rsa_key: |\n    -----BEGIN OPENSSH PRIVATE KEY-----\n    <FILL ME>\n    -----END OPENSSH PRIVATE KEY-----\n  ssh_host_rsa_key.pub: |\n    ssh-rsa <FILL ME>\n  sshd_config: |\n    Port 22\n    AddressFamily any\n    ListenAddress 0.0.0.0\n    ListenAddress ::\n\n    HostKey /etc/ssh/ssh_host_rsa_key\n    HostKey /etc/ssh/ssh_host_ecdsa_key\n    HostKey /etc/ssh/ssh_host_ed25519_key\n\n    PermitRootLogin prohibit-password\n    PasswordAuthentication yes\n\n    # Change to yes to enable challenge-response passwords (beware issues with\n    # some PAM modules and threads)\n    ChallengeResponseAuthentication no\n\n    UsePAM yes\n\n    X11Forwarding yes\n    PrintMotd no\n    AcceptEnv LANG LC_*\n\n    # override default of no subsystems\n    Subsystem sftp\t/usr/lib/openssh/sftp-server\n\n    AuthorizedKeysCommand /usr/bin/sss_ssh_authorizedkeys\n    AuthorizedKeysCommandUser root\n"})}),"\n",(0,l.jsxs)(n.p,{children:["Replace the ",(0,l.jsx)(n.code,{children:"<FILL ME>"})," with the values based on the generated files."]}),"\n",(0,l.jsxs)(n.ol,{start:"2",children:["\n",(0,l.jsx)(n.li,{children:"Seal the secret:"}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",metastring:'title="user@local:/ClusterFactory"',children:"cfctl kubeseal\n"})}),"\n",(0,l.jsxs)(n.ol,{start:"3",children:["\n",(0,l.jsx)(n.li,{children:"Apply the SealedSecret:"}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",metastring:'title="user@local:/ClusterFactory"',children:"kubectl apply -f argo/slurm-cluster/secrets/login-sshd-sealed-secret.yaml\n"})}),"\n",(0,l.jsx)(n.h4,{id:"home-directory-for-the-ldap-users",children:"Home directory for the LDAP users"}),"\n",(0,l.jsxs)(n.p,{children:["If you have configured your LDAP server, you might have to change the ",(0,l.jsx)(n.code,{children:"homeDirectory"})," to something like ",(0,l.jsx)(n.code,{children:"/home/ldap-users"}),"."]}),"\n",(0,l.jsx)(n.p,{children:"You must mount the home directory of the LDAP users using NFS."}),"\n",(0,l.jsxs)(n.p,{children:["DO NOT use ",(0,l.jsx)(n.code,{children:"StorageClass"})," since the provisioning is static. We don't want to create a volume per replica. There is only one common volume."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-yaml",metastring:'title="argo/slurm-cluster/volumes/ldap-users-<cluster name>-pv.yaml"',children:"apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: ldap-users-<cluster name>-pv\n  namespace: slurm-cluster\n  labels:\n    app: slurm-login\n    topology.kubernetes.io/region: <FILL ME> # <country code>-<city>\n    topology.kubernetes.io/zone: <FILL ME> # <country code>-<city>-<index>\nspec:\n  capacity:\n    storage: 1000Gi\n  mountOptions:\n    - hard\n    - nfsvers=4.1\n    - noatime\n    - nodiratime\n  csi:\n    driver: nfs.csi.k8s.io\n    readOnly: false\n    volumeHandle: <unique id> # uuidgen\n    volumeAttributes:\n      server: <FILL ME> # IP or host\n      share: <FILL ME> # /srv/nfs/k8s/ldap-users\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: ldap-users-<cluster name>-pvc\n  namespace: slurm-cluster\n  labels:\n    app: slurm-login\n    topology.kubernetes.io/region: <FILL ME> # <country code>-<city>\n    topology.kubernetes.io/zone: <FILL ME> # <country code>-<city>-<index>\nspec:\n  volumeName: ldap-users-<cluster name>-pv\n  accessModes: [ReadWriteMany]\n  storageClassName: ''\n  resources:\n    requests:\n      storage: 1000Gi\n"})}),"\n",(0,l.jsx)(n.p,{children:"Apply the PV and PVC:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",metastring:'title="user@local:/ClusterFactory"',children:"kubectl apply -f argo/slurm-cluster/volumes/ldap-users-<cluster name>-pv.yaml\n"})}),"\n",(0,l.jsx)(n.h3,{id:"6b-values-enable-slurm-login",children:"6.b. Values: Enable SLURM Login"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-yaml",metastring:'title="helm/slurm-cluster/values-<cluster name>.yaml"',children:"login:\n  enabled: true\n  replicas: 1\n\n  sshd:\n    secretName: login-sshd-secret\n\n  nodeSelector:\n    topology.kubernetes.io/region: <FILL ME> # <country code>-<city>\n    topology.kubernetes.io/zone: <FILL ME> # <country code>-<city>-<index>\n\n  # Extra volume mounts\n  volumeMounts:\n    - name: ldap-users-pvc\n      mountPath: /home/ldap-users\n\n  # Extra volumes\n  volumes:\n    - name: ldap-users-pvc\n      persistentVolumeClaim:\n        claimName: ldap-users-<cluster name>-pvc\n\n  service:\n    enabled: true\n    type: ClusterIP\n    # Use LoadBalancer to expose via MetalLB\n    # type: LoadBalancer\n\n    # annotations:\n    #   metallb.universe.tf/address-pool: slurm-ch-basel-1-pool\n\n  # Expose via IPVLAN, can be unstable.\n  # Using IPVLAN permits srun commands.\n  net:\n    enabled: false\n    # Kubernetes host interface\n    type: ipvlan\n    masterInterface: eth0\n    mode: l2\n\n    # https://www.cni.dev/plugins/current/ipam/static/\n    ipam:\n      type: static\n      addresses:\n        - address: 192.168.0.20/24\n          gateway: 192.168.0.1\n"})}),"\n",(0,l.jsx)(n.p,{children:"Edit the values accordingly."}),"\n",(0,l.jsxs)(n.admonition,{type:"warning",children:[(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Service or IPVLan?"})}),(0,l.jsx)(n.p,{children:"A Kubernetes Service offers a lot of advantages while IPVLan offers a solution to a problem."}),(0,l.jsxs)(n.p,{children:["It is ",(0,l.jsx)(n.strong,{children:"extremely"})," recommended to use a Kubernetes service to expose your connection node as it provides load balancing and is easy to configure."]}),(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Kubernetes LoadBalancer Service"}),(0,l.jsx)(n.th,{children:"Multus CNI"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"A LoadBalancer service provides limited control over networking, as it only provides a single IP address for a Kubernetes service."}),(0,l.jsx)(n.td,{children:"IPVLan with Multus allows you to have more fine-grained control over networking by enabling you to use multiple network interfaces in a pod, each with its own IP address and route table."})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"A LoadBalancer service is a simple and straightforward way to expose a Kubernetes service to the internet."}),(0,l.jsx)(n.td,{children:"Setting up IPVLan with Multus can be more complex than using a simple LoadBalancer service, as it requires more configuration and setup time."})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"A LoadBalancer service can only expose a set of ports."}),(0,l.jsx)(n.td,{children:"Using IPVLan with Multus will allow a pod to directly connect to the host network."})]})]})]}),(0,l.jsx)(n.p,{children:(0,l.jsxs)(n.strong,{children:["As a result, using a Kubernetes LoadBalancer service will render the Slurm ",(0,l.jsx)(n.code,{children:"srun"})," commands inoperable (although ",(0,l.jsx)(n.code,{children:"sbatch"})," will work and is the preferred method for job submission). On the other hand, adopting Multus CNI eliminates the load balancing feature, but could lead to instability."]})})]}),"\n",(0,l.jsxs)(n.admonition,{type:"warning",children:[(0,l.jsxs)(n.p,{children:["Because ",(0,l.jsx)(n.code,{children:"k8s-pod-network"})," is the default network, you must write routes to your networks."]}),(0,l.jsxs)(n.p,{children:["For example, if we have two sites ",(0,l.jsx)(n.code,{children:"10.10.0.0/24"})," and ",(0,l.jsx)(n.code,{children:"10.10.1.0/24"}),", you would write:"]}),(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-yaml",children:"ipam:\n  type: static\n  addresses:\n    - address: 192.168.0.20/24\n      gateway: 192.168.0.1\n  routes:\n    - dst: 10.10.1.0/24\n"})}),(0,l.jsxs)(n.p,{children:["If we ",(0,l.jsx)(n.code,{children:"kubectl exec"})," to the container and run ",(0,l.jsx)(n.code,{children:"ip route"}),", you would see:"]}),(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",metastring:'title="root@slurm-cluster-e<cluster name>-login-b9b7cd9d5-9ntkn"',children:"# ip route\ndefault via 169.254.1.1 dev eth0\n10.10.0.0/24 via 10.10.0.1 dev net1\n10.10.1.0/24 via 10.10.0.1 dev net1\n169.254.1.1 dev eth0 scope link\n10.10.0.0/20 via 10.10.0.1 dev net1\n10.10.0.0/20 dev net1 proto kernel scope link src 10.10.0.21\n"})}),(0,l.jsxs)(n.p,{children:["The issue is tracked at ",(0,l.jsx)(n.a,{href:"https://github.com/deepsquare-io/ClusterFactory/issues/29",children:"deepsquare-io/ClusterFactory#29"})," and ",(0,l.jsx)(n.a,{href:"https://github.com/projectcalico/calico/issues/5199",children:"projectcalico/calico#5199"}),"."]})]}),"\n",(0,l.jsx)(n.p,{children:"If you are using LDAPS and the CA is private, add these values:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-yaml",metastring:'title="helm/slurm-cluster/values-<cluster name>.yaml"',children:"login:\n  # ...\n  command: ['sh', '-c', 'update-ca-trust && /init']\n\n  volumeMounts:\n    - name: ldap-users-pvc\n      mountPath: /home/ldap-users\n    - name: ca-cert\n      mountPath: /etc/pki/ca-trust/source/anchors/example.com.ca.pem\n      subPath: example.com.ca.pem\n\n  volumes:\n    - name: ldap-users-pvc\n      persistentVolumeClaim:\n        claimName: ldap-users-<cluster name>-pvc\n    - name: ca-cert\n      secret:\n        secretName: local-ca-secret\n"})}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.code,{children:"local-ca-secret"})," is a Secret containing ",(0,l.jsx)(n.code,{children:"example.com.ca.pem"}),"."]}),"\n",(0,l.jsx)(n.p,{children:"You can deploy it:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",children:'git add .\ngit commit -m "Added SLURM Login values"\ngit push\n'})}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",metastring:'title="user@local:/ClusterFactory"',children:"# This is optional if the application is already deployed.\nkubectl apply -f argo/slurm-cluster/apps/slurm-cluster-<cluster name>-app.yaml\n"})}),"\n",(0,l.jsx)(n.h3,{id:"6c-testing-access-to-a-slurm-login-node",children:"6.c Testing: Access to a SLURM Login node"}),"\n",(0,l.jsx)(n.p,{children:"Because the container is exposed to the external network, you should be able to ssh directly to the login node."}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-shell",metastring:'title="user@local"',children:"ssh user@login-node\n"})}),"\n",(0,l.jsxs)(n.p,{children:["If the LDAP User ",(0,l.jsx)(n.code,{children:"user"})," exists, the login node should be asking for a password."]})]})}function u(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(d,{...e})}):d(e)}},1538:(e,n,s)=>{s.d(n,{A:()=>S});var r=s(6672),l=s(3526),t=s(9914),a=s(5291),o=s(1066),i=s(4319),c=s(8004),d=s(8379);function u(e){return r.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function h(e){const{values:n,children:s}=e;return(0,r.useMemo)((()=>{const e=n??function(e){return u(e).map((({props:{value:e,label:n,attributes:s,default:r}})=>({value:e,label:n,attributes:s,default:r})))}(s);return function(e){const n=(0,c.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,s])}function m({value:e,tabValues:n}){return n.some((n=>n.value===e))}function p({queryString:e=!1,groupId:n}){const s=(0,a.W6)(),l=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,i.aZ)(l),(0,r.useCallback)((e=>{if(!l)return;const n=new URLSearchParams(s.location.search);n.set(l,e),s.replace({...s.location,search:n.toString()})}),[l,s])]}function g(e){const{defaultValue:n,queryString:s=!1,groupId:l}=e,t=h(e),[a,i]=(0,r.useState)((()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const s=n.find((e=>e.default))??n[0];if(!s)throw new Error("Unexpected error: 0 tabValues");return s.value}({defaultValue:n,tabValues:t}))),[c,u]=p({queryString:s,groupId:l}),[g,x]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[s,l]=(0,d.Dv)(n);return[s,(0,r.useCallback)((e=>{n&&l.set(e)}),[n,l])]}({groupId:l}),y=(()=>{const e=c??g;return m({value:e,tabValues:t})?e:null})();(0,o.A)((()=>{y&&i(y)}),[y]);return{selectedValue:a,selectValue:(0,r.useCallback)((e=>{if(!m({value:e,tabValues:t}))throw new Error(`Can't select invalid tab value=${e}`);i(e),u(e),x(e)}),[u,x,t]),tabValues:t}}var x=s(2146);const y={tabList:"tabList_iqnf",tabItem:"tabItem_apUM"};var j=s(3420);function f({className:e,block:n,selectedValue:s,selectValue:r,tabValues:a}){const o=[],{blockElementScrollPositionUntilNextRender:i}=(0,t.a_)(),c=e=>{const n=e.currentTarget,l=o.indexOf(n),t=a[l].value;t!==s&&(i(n),r(t))},d=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const s=o.indexOf(e.currentTarget)+1;n=o[s]??o[0];break}case"ArrowLeft":{const s=o.indexOf(e.currentTarget)-1;n=o[s]??o[o.length-1];break}}n?.focus()};return(0,j.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,l.A)("tabs",{"tabs--block":n},e),children:a.map((({value:e,label:n,attributes:r})=>(0,j.jsx)("li",{role:"tab",tabIndex:s===e?0:-1,"aria-selected":s===e,ref:e=>{o.push(e)},onKeyDown:d,onClick:c,...r,className:(0,l.A)("tabs__item",y.tabItem,r?.className,{"tabs__item--active":s===e}),children:n??e},e)))})}function b({lazy:e,children:n,selectedValue:s}){const t=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=t.find((e=>e.props.value===s));return e?(0,r.cloneElement)(e,{className:(0,l.A)("margin-top--md",e.props.className)}):null}return(0,j.jsx)("div",{className:"margin-top--md",children:t.map(((e,n)=>(0,r.cloneElement)(e,{key:n,hidden:e.props.value!==s})))})}function v(e){const n=g(e);return(0,j.jsxs)("div",{className:(0,l.A)("tabs-container",y.tabList),children:[(0,j.jsx)(f,{...n,...e}),(0,j.jsx)(b,{...n,...e})]})}function S(e){const n=(0,x.A)();return(0,j.jsx)(v,{...e,children:u(e.children)},String(n))}},2094:(e,n,s)=>{s.d(n,{A:()=>r});const r=s.p+"assets/images/image-20220512151655613-a88d0547e2bc1fb1d959aa9d46383a2e.png"},6267:(e,n,s)=>{s.d(n,{A:()=>a});s(6672);var r=s(3526);const l={tabItem:"tabItem_XLTN"};var t=s(3420);function a({children:e,hidden:n,className:s}){return(0,t.jsx)("div",{role:"tabpanel",className:(0,r.A)(l.tabItem,s),hidden:n,children:e})}}}]);